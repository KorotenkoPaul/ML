{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([2, 4, 7, 9])\n",
    "T = np.array([3, 5, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(T, Y):\n",
    "    \"\"\"\n",
    "    Returns mean squared error.\n",
    "    \n",
    "    Y is a vector of predictions\n",
    "    T is a vector of true answers\n",
    "    \"\"\"\n",
    "    \n",
    "    # We could instead do:\n",
    "    # import from sklearn.metrics import mean_squared_error as mse\n",
    "    return ((Y - T) ** 2).sum() / len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BGD(X, T, epochs=100, lr=0.01):\n",
    "    \"\"\"\n",
    "    Batch gradient descent for a linear regression model y = kx + b\n",
    "    with only one input feature.\n",
    "    \n",
    "    X is a vector of all training set inputs\n",
    "    T is a vector of all respective true answers\n",
    "    lr stands for learning rate\n",
    "    epochs is a number of iterations over all samples of training set\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initializing slope and intercept with random values\n",
    "    k, b = np.random.sample(2)\n",
    "    # Number of training samples\n",
    "    n = len(X)\n",
    "\n",
    "    # Training the model\n",
    "    for _ in range(epochs):\n",
    "        # Calculating our linear regression prediction vector\n",
    "        # so that we could later use it when find gradient\n",
    "        # of the cost function over all training samples\n",
    "        Y = k * X + b\n",
    "        \n",
    "        # Computing gradient itself\n",
    "        delta_k = (2 * X * (Y - T)).sum() / n\n",
    "        delta_b = (2 * (Y - T)).sum() / n\n",
    "        \n",
    "        # Updating slope and intercept by substracting gradient\n",
    "        # multiplied by learning rate\n",
    "        k -= lr * delta_k\n",
    "        b -= lr * delta_b\n",
    "    \n",
    "    # We pass k*X+b instead of just passing Y because we want\n",
    "    # to see the error calculated for the latest parameters k and b\n",
    "    # And Y is computed before updating parameters\n",
    "    print(f'MSE: {mse(T, k*X+b)}')\n",
    "    \n",
    "    return k, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.23734691216242063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3676579468340696, 2.720706302376377)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BGD(X, T, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(X, T, epochs=100, lr=0.01, batch_size=1):\n",
    "    \"\"\"\n",
    "    Note that for every iteration we randomly choose a batch of samples\n",
    "    and move on to next iteration. We don't choose consecutive bathes\n",
    "    from training set till it ends. That's why we could train our linear\n",
    "    regression model more on some samples and less on the others.\n",
    "    \n",
    "    Changing batch_size to the value other than 1 turns SGD\n",
    "    into mini-batch gradient descent.\n",
    "    \n",
    "    If we were to increase batch_size up to the length of our training set,\n",
    "    that wouldn't mean SGD turns into BGD because indexes are chosen randomly\n",
    "    and the same index can appear multiple times in our array of indexes.\n",
    "    That's why we might train our model multiple times on the same samples\n",
    "    during a single iteration.\n",
    "    \n",
    "    Moreover we could increase batch_size to the value bigger than the length\n",
    "    of our training set.\n",
    "    \n",
    "    Since we're updating our weights for each training sample, the output\n",
    "    of our model changes for each sample as well (because it uses new k\n",
    "    and b values). Hence each sample cost's gradient is calculated with different\n",
    "    model's output y in contrast to BGD where each sample cost's gradient before\n",
    "    it's summed is calculated with the same output y of the model.\n",
    "    \n",
    "    That's why even if we were to consecutively take inputs from training set\n",
    "    till the whole set is used we wouldn't get the same gradient for the set\n",
    "    as we do when we use BGD (because of updating weights for each sample which\n",
    "    causes our model's output to change before we use this output in the next\n",
    "    sample cost's gradient calculation).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initializing slope and intercept with random values\n",
    "    k, b = np.random.sample(2)\n",
    "\n",
    "    # Training the model\n",
    "    for _ in range(epochs):\n",
    "        # Forming an array of batch_size length of random indexes\n",
    "        # where each index is in interval [0; len(X))\n",
    "        indexes = np.random.randint(0, len(X), batch_size)\n",
    "        \n",
    "        # Choosing a batch of random inputs\n",
    "        batch = np.take(X, indexes)\n",
    "        # Choosing respective answers\n",
    "        answers = np.take(T, indexes)\n",
    "        \n",
    "        # Calculating our linear regression prediction vector\n",
    "        # so that we could later use it when find gradient\n",
    "        # of the cost function over our batch samples\n",
    "        Y = k * batch + b\n",
    "        \n",
    "        # Computing gradient itself\n",
    "        delta_k = (2 * batch * (Y - answers)).sum() / batch_size\n",
    "        delta_b = (2 * (Y - answers)).sum() / batch_size\n",
    "        \n",
    "        # Updating slope and intercept by substracting gradient\n",
    "        # multiplied by learning rate\n",
    "        k -= lr * delta_k\n",
    "        b -= lr * delta_b\n",
    "    \n",
    "    print(f'MSE: {mse(T, k*X+b)}')\n",
    "    \n",
    "    return k, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2898347910172201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4134318648443603, 2.6595348211929317)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGD(X, T, epochs=1000, batch_size=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
